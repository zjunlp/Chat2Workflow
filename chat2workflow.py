import chainlit as cl
from chainlit.input_widget import TextInput, Slider
import json
import os
import re
import asyncio
from datetime import datetime

# å¯¼å…¥ä½ åŸæœ‰çš„æ ¸å¿ƒé€»è¾‘
from llm_api import OpenAIAgent
from pass_stage import convert_to_yaml

# --- å…¨å±€çŠ¶æ€ç¼“å­˜ ---
# ä»…ä¿ç•™ settingsï¼Œç§»é™¤ chat_historyï¼Œä»¥å®ç°æ¯æ¬¡æ–°å»ºå¯¹è¯éƒ½æ˜¯å…¨æ–°çš„ä¸Šä¸‹æ–‡
GLOBAL_STATE = {
    "settings": {
        "model_name": "deepseek-chat",
        "temperature": 0.7,
        "max_tokens": 8192
    }
}

# --- æ ¸å¿ƒæ•°æ®å¤„ç†å‡½æ•° ---

def load_system_prompt():
    prompt_path = "prompts/builder_prompt.txt"
    if os.path.exists(prompt_path):
        with open(prompt_path, 'r', encoding='utf-8') as f:
            return f.read().strip()
    return "You are a helpful AI assistant for workflow generation."

def extract_workflow_json(text):
    pattern = r'<workflow>(.*?)</workflow>'
    match = re.search(pattern, text, re.DOTALL | re.IGNORECASE)
    if match:
        workflow_str = match.group(1).strip()
        try:
            json.loads(workflow_str)
            return workflow_str, True, None
        except json.JSONDecodeError as e:
            return workflow_str, False, f"Invalid JSON: {str(e)}"
    return None, False, "No <workflow> tags found"

def save_workflow_yaml(workflow_json_str, task_name=None):
    if task_name is None:
        task_name = f"workflow_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
    
    yaml_dir = "output/generated_workflows"
    os.makedirs(yaml_dir, exist_ok=True)
    
    try:
        success = convert_to_yaml(workflow_json_str, task_name, 1, yaml_dir)
        if success:
            yaml_path = os.path.join(yaml_dir, f"{task_name}_1.yaml")
            return True, yaml_path, task_name
        return False, None, "Conversion failed"
    except Exception as e:
        return False, None, f"Error during conversion: {str(e)}"


# --- Chainlit UI äº¤äº’é€»è¾‘ ---

@cl.on_chat_start
async def start():
    """åˆå§‹åŒ–é¡µé¢ï¼Œæ¢å¤é…ç½®å‚æ•°ï¼Œä½†é‡ç½®å†å²ä¸Šä¸‹æ–‡"""
    saved_settings = GLOBAL_STATE["settings"]
    
    # 1. è®¾ç½®å¹¶æ¢å¤ä¾§è¾¹æ å‚æ•°é…ç½®
    settings = cl.ChatSettings(
        [
            TextInput(id="model_name", label="Model Name", initial=saved_settings["model_name"]),
            Slider(id="temperature", label="Temperature", initial=saved_settings["temperature"], min=0.0, max=2.0, step=0.1),
            Slider(id="max_tokens", label="Max Tokens", initial=saved_settings["max_tokens"], min=512, max=16384, step=512)
        ]
    )
    await settings.send()
    
    # 2. åˆå§‹åŒ– Agent
    try:
        system_prompt = load_system_prompt()
        agent = OpenAIAgent(
            model_name=saved_settings["model_name"],
            system_prompt=system_prompt,
            temperature=saved_settings["temperature"],
            max_tokens=saved_settings["max_tokens"]
        )
        cl.user_session.set("agent", agent)
    except Exception as e:
        await cl.Message(content=f"âŒ Agent åˆå§‹åŒ–å¤±è´¥: {str(e)}").send()
    
    # 3. åˆå§‹åŒ–å…¨æ–°çš„å†å²å¯¹è¯è®°å½•ï¼ˆä¸å†ä» GLOBAL_STATE è·å–ï¼‰
    cl.user_session.set("chat_history", [])
    
    # 4. å‘é€æ¬¢è¿è¯­åŠçŠ¶æ€æç¤º
    welcome_msg = f"ğŸ‘‹ **æ¬¢è¿ä½¿ç”¨ Chat2Workflowï¼**\n\n\nmodel: `{saved_settings['model_name']}`, temperature: `{saved_settings['temperature']}`, max_tokens: `{saved_settings['max_tokens']}`"
        
    await cl.Message(content=welcome_msg).send()

@cl.on_settings_update
async def setup_agent(settings):
    """å½“ç”¨æˆ·ä¿®æ”¹è®¾ç½®æ—¶ï¼Œæ›´æ–°å…¨å±€çŠ¶æ€å¹¶é‡æ–°åŠ è½½ Agent"""
    try:
        # åŒæ­¥æ›´æ–°åˆ°å…¨å±€çŠ¶æ€
        GLOBAL_STATE["settings"]["model_name"] = settings["model_name"]
        GLOBAL_STATE["settings"]["temperature"] = settings["temperature"]
        GLOBAL_STATE["settings"]["max_tokens"] = settings["max_tokens"]
        
        system_prompt = load_system_prompt()
        agent = OpenAIAgent(
            model_name=settings["model_name"],
            system_prompt=system_prompt,
            temperature=settings["temperature"],
            max_tokens=settings["max_tokens"]
        )
        cl.user_session.set("agent", agent)
        
        # æ˜ç¡®æç¤ºå½“å‰æ›´æ–°çš„æ¨¡å‹åç§°
        await cl.Message(content=f"âœ… é…ç½®å·²æ›´æ–°ï¼\n\n\nmodel: `{settings['model_name']}`, temperature: `{settings['temperature']}`, max_tokens: `{settings['max_tokens']}`").send()
    except Exception as e:
        await cl.Message(content=f"âŒ æ›´æ–°å¤±è´¥: {str(e)}").send()

@cl.on_message
async def main(message: cl.Message):
    """å¤„ç†ç”¨æˆ·æ¶ˆæ¯æ ¸å¿ƒé€»è¾‘"""
    agent = cl.user_session.get("agent")
    chat_history = cl.user_session.get("chat_history")
    
    if not agent:
        await cl.Message(content="âš ï¸ Agent æœªå°±ç»ªï¼Œè¯·åˆ·æ–°é¡µé¢ã€‚").send()
        return

    msg = cl.Message(content="")
    await msg.send()
    
    full_reasoning = ""
    full_response = ""
    
    has_reasoning = False
    reasoning_closed = False

    try:
        # æµå¼è·å– Agent è¾“å‡º
        for reasoning_chunk, content_chunk in agent.generate_stream(query=message.content, history=chat_history):
            
            # å¤„ç†æ€è€ƒè¿‡ç¨‹ï¼šä½¿ç”¨ Markdown çš„æŠ˜å æ ‡ç­¾ (open å±æ€§ä»£è¡¨é»˜è®¤å±•å¼€)
            if reasoning_chunk:
                if not has_reasoning:
                    await msg.stream_token("ğŸ§  æ€è€ƒè¿‡ç¨‹\n\n> ")
                    has_reasoning = True
                
                # ä¸ºäº†è§†è§‰ç¾è§‚ï¼Œæ€è€ƒå†…å®¹åŠ ä¸Šå¼•ç”¨å—çš„æ ¼å¼
                clean_chunk = reasoning_chunk.replace('\n', '\n> ')
                full_reasoning += reasoning_chunk
                await msg.stream_token(clean_chunk)
            
            # å¤„ç†æœ€ç»ˆå›å¤å†…å®¹
            if content_chunk:
                # å¦‚æœä¹‹å‰æœ‰æ€è€ƒè¿‡ç¨‹ï¼Œä¸”å°šæœªé—­åˆæ ‡ç­¾ï¼Œåˆ™åœ¨æ­¤å¤„é—­åˆ
                if has_reasoning and not reasoning_closed:
                    await msg.stream_token("\n\n---\n\n")
                    reasoning_closed = True
                
                full_response += content_chunk
                await msg.stream_token(content_chunk)
            
            await asyncio.sleep(0.01)
            
    except Exception as e:
        await cl.Message(content=f"âŒ ç”Ÿæˆè¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}").send()
        return

    # å®¹é”™æ”¶å°¾ï¼šå¦‚æœæ¨¡å‹åªæœ‰æ€è€ƒæ²¡è¾“å‡ºæ­£æ–‡ï¼Œç¡®ä¿æ ‡ç­¾é—­åˆ
    if has_reasoning and not reasoning_closed:
        await msg.stream_token("\n</details>\n\n")
        
    await msg.update()

    # --- æ›´æ–°å¯¹è¯å†å²ï¼ˆä»…ä¿å­˜åœ¨å½“å‰ sessionï¼Œä¸å†åŒæ­¥åˆ°å…¨å±€çŠ¶æ€ï¼‰ ---
    chat_history.append((message.content, full_response))
    cl.user_session.set("chat_history", chat_history)

    # --- æå– JSON å¹¶å°è¯•è½¬æ¢ä¸º YAML ---
    workflow_json, is_valid, error_msg = extract_workflow_json(full_response)
    
    if workflow_json and is_valid:
        success, yaml_path, task_name = save_workflow_yaml(workflow_json)
        
        if success and yaml_path and os.path.exists(yaml_path):
            with open(yaml_path, 'r', encoding='utf-8') as f:
                yaml_content = f.read()
            
            yaml_preview = cl.Text(
                name="YAML é¢„è§ˆ", 
                content=yaml_content, 
                language="yaml", 
                display="side"
            )
            yaml_download = cl.File(
                name=f"{task_name}.yaml",
                content=yaml_content.encode('utf-8'),
                display="inline"
            )
            
            await cl.Message(
                content=f"ğŸ‰ **å·¥ä½œæµ YAML å·²ç”Ÿæˆï¼**",
                elements=[yaml_preview, yaml_download]
            ).send()
        else:
            await cl.Message(content=f"âš ï¸ YAML è½¬æ¢å¤±è´¥ã€‚é”™è¯¯: {error_msg}").send()
            
    elif workflow_json and not is_valid:
        await cl.Message(content=f"âŒ å‘ç°å·¥ä½œæµæ ‡ç­¾ä½† JSON æ ¼å¼é”™è¯¯: `{error_msg}`").send()